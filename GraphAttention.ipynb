{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "491c4eea-ac2e-4369-979b-de6a643ce3ff",
   "metadata": {},
   "source": [
    "# Graph Neural Networks (GNNs)\n",
    "\n",
    "Graph Neural Networks are a class of deep learning models designed to perform inference on **graph-structured data**. They generalize traditional neural networks to handle non-Euclidean data where relationships between entities are explicitly defined by edges.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1. Graph Basics\n",
    "- **Nodes/Vertices (V)**: Represent entities (e.g., users in social networks, atoms in molecules)\n",
    "- **Edges (E)**: Represent relationships between nodes\n",
    "- **Node Features**: Attributes associated with each node (e.g., user profiles, atom properties)\n",
    "- **Adjacency Matrix (A)**: Matrix representation of edge connections\n",
    "\n",
    "### 2. Core GNN Operations\n",
    "#### Message Passing\n",
    "GNNs operate via **message passing** where nodes:\n",
    "1. **Aggregate** information from neighbors\n",
    "2. **Update** their own representation based on aggregated messages\n",
    "\n",
    "#### Common Variants\n",
    "- **Graph Convolutional Networks (GCNs)**\n",
    "- **Graph Attention Networks (GATs)**\n",
    "- **GraphSAGE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68eceb0-dc1a-4405-8484-38b0f872dd5b",
   "metadata": {},
   "source": [
    "### A basic GNN (following the Message Passing framework) includes:\n",
    "\n",
    "1. Message function: How a node collects information from neighbors.\n",
    "2. Aggregation function: Combines messages from all neighbors (e.g., sum, mean).\n",
    "3. Update function: Updates node's own features using the aggregated message.\n",
    "\n",
    "\n",
    "- [x] General GNN (Message Passing Framework)\n",
    "\n",
    "  $m_v^{(l)} = \\text{AGGREGATE}(\\{h_u^{(l)} : u \\in N(v)\\})$\n",
    "\n",
    "  $h_v^{(l+1)} = \\text{UPDATE}(h_v^{(l)}, m_v^{(l)})$\n",
    "\n",
    "We'll implement three typical aggregators:\n",
    "\n",
    "- Mean\n",
    "- Sum\n",
    "- Max\n",
    "\n",
    "We’ll:\n",
    "\n",
    "1. Collect neighbor features explicitly\n",
    "2. Apply a chosen aggregation function (mean, sum, or max)\n",
    "3. Combine it with the node’s own features\n",
    "\n",
    "This simulates message passing and custom aggregation — hallmarks of general GNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5c0cca0-f8d8-44f0-91e7-8d5d1d370aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f98b03-aee2-4150-8c18-5314bae75a1b",
   "metadata": {},
   "source": [
    "#### Aggregation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e074de73-3ed8-4b47-aaec-dd3b6b1cacda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_neighbors(X, A, mode=\"mean\"):\n",
    "    N = A.size(0)\n",
    "    X_neighbors = torch.zeros_like(X)\n",
    "\n",
    "    for i in range(N):\n",
    "        neighbors = A[i].nonzero().squeeze()\n",
    "        if neighbors.ndim == 0:\n",
    "            neighbors = neighbors.unsqueeze(0)\n",
    "\n",
    "        if len(neighbors) == 0:\n",
    "            agg = torch.zeros(X.shape[1])\n",
    "        else:\n",
    "            neigh_feats = X[neighbors]\n",
    "            if mode == \"mean\":\n",
    "                agg = neigh_feats.mean(dim=0)\n",
    "            elif mode == \"sum\":\n",
    "                agg = neigh_feats.sum(dim=0)\n",
    "            elif mode == \"max\":\n",
    "                agX_neighborsg = neigh_feats.max(dim=0)[0]\n",
    "            else:\n",
    "                raise ValueError(\"Unknown aggregation mode\")\n",
    "\n",
    "        X_neighbors[i] = agg\n",
    "    return X_neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bcf28c-0292-47ea-9538-2adf08cd7d7a",
   "metadata": {},
   "source": [
    "#### GNN Layer with Custom Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4953205a-276e-4589-bd00-b054592dbe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralGNNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, agg_mode=\"mean\"):\n",
    "        super(GeneralGNNLayer, self).__init__()\n",
    "        self.W_self = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.W_neigh = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.agg_mode = agg_mode\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        # Aggregate from neighbors using custom aggregator\n",
    "        neighbor_agg = aggregate_neighbors(X, A, self.agg_mode)\n",
    "        \n",
    "        # Linear transform for self and neighbors\n",
    "        h_self = X @ self.W_self\n",
    "        h_neigh = neighbor_agg @ self.W_neigh\n",
    "\n",
    "        # Combine and activate\n",
    "        return F.relu(h_self + h_neigh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b699d-64dc-4a57-bee2-7d6b815e948f",
   "metadata": {},
   "source": [
    "#### Full 2-Layer GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c4f9f7c-9c68-44a0-a7b7-0841ca9caa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralGNN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, out_features, agg_mode=\"mean\"):\n",
    "        super(GeneralGNN, self).__init__()\n",
    "        self.layer1 = GeneralGNNLayer(in_features, hidden_dim, agg_mode)\n",
    "        self.layer2 = GeneralGNNLayer(hidden_dim, out_features, agg_mode)\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        x = self.layer1(X, A)\n",
    "        x = self.layer2(x, A)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73173725-961c-4047-980b-d6db089b0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([\n",
    "    [1.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 1.0],\n",
    "    [0.0, 0.0]\n",
    "])\n",
    "\n",
    "A = torch.tensor([\n",
    "    [0, 1, 0, 0],\n",
    "    [1, 0, 1, 0],\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 0, 1, 0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "labels = torch.tensor([0, 1, 0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e10d5000-361d-4269-a89c-3f4fdb4a9edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 1.0915 | Accuracy: 0.0000\n",
      "Epoch 10 | Loss: 0.8451 | Accuracy: 0.5000\n",
      "Epoch 20 | Loss: 0.7184 | Accuracy: 0.5000\n",
      "Epoch 30 | Loss: 0.6838 | Accuracy: 0.5000\n",
      "Epoch 40 | Loss: 0.6651 | Accuracy: 0.5000\n",
      "Epoch 50 | Loss: 0.6329 | Accuracy: 0.5000\n",
      "Epoch 60 | Loss: 0.5772 | Accuracy: 0.5000\n",
      "Epoch 70 | Loss: 0.5117 | Accuracy: 1.0000\n",
      "Epoch 80 | Loss: 0.4572 | Accuracy: 1.0000\n",
      "Epoch 90 | Loss: 0.4174 | Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = GeneralGNN(in_features=2, hidden_dim=4, out_features=2, agg_mode=\"max\")  # Try \"sum\" or \"mean\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(X, A)\n",
    "    loss = loss_fn(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        pred = out.argmax(dim=1)\n",
    "        acc = (pred == labels).float().mean().item()\n",
    "        print(f\"Epoch {epoch} | Loss: {loss.item():.4f} | Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb5571-a63e-4576-862c-ddd2ff7404d5",
   "metadata": {},
   "source": [
    "##### Matrix multiplication (e.g., $A \\cdot X$) is how nodes aggregate information from their neighbors in a GNN.\n",
    "\n",
    "#### Specifically:\n",
    "- $A \\in \\mathbb{R}^{N \\times N}$: adjacency matrix\n",
    "- $A_{ij} = 1$ if node $i$ is connected to node $j$\n",
    "- $X \\in \\mathbb{R}^{N \\times d}$: node features (each row = node’s feature vector)\n",
    "- $A \\cdot X \\rightarrow$ Each node’s new feature is the sum of its neighbors’ features\n",
    "\n",
    "#### Why it matters:\n",
    "- It enables message passing: collecting info from neighbors\n",
    "- When you multiply $A \\cdot X$, you’re computing:\n",
    "  $\n",
    "  (A X)_i = \\sum_{j \\in N(i)} X_j\n",
    "  $\n",
    "- If $A$ is normalized, it performs mean pooling (e.g., GCN-style)\n",
    "- Matrix multiplication = fast, vectorized aggregation across all nodes."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### simple GNN",
   "id": "3ec285a4f4bc7c71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdab48fc-1295-46d3-8176-4f825af3f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"Implementation of a basic GNN layer from scratch\"\"\"\n",
    "        super().__init__()\n",
    "        # Weight matrix for neighbor aggregation\n",
    "        self.neighbor_weights = nn.Linear(input_dim, output_dim)\n",
    "        # Weight matrix for self node features\n",
    "        self.self_weights = nn.Linear(input_dim, output_dim)\n",
    "        # Bias term\n",
    "        self.bias = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "    def forward(self, X, adj):\n",
    "        \"\"\"\n",
    "        X: Node features [num_nodes, input_dim]\n",
    "        adj: Normalized adjacency matrix [num_nodes, num_nodes]\n",
    "        \"\"\"\n",
    "        # aggregate neighbour inforamtion\n",
    "        neighbour_info = torch.matmul(adj, self.neighbor_weights(X))\n",
    "        # include self info\n",
    "        self_info = self.self_weights(X)\n",
    "        # combine and add bias\n",
    "        output = neighbour_info + self_info + self.bias\n",
    "        return F.relu(output)\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    \"\"\"Two-layer GNN implementation\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = GNNLayer(input_dim, hidden_dim)\n",
    "        self.layer2 = GNNLayer(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce14acb-947c-4acf-80d3-f3cd26dadc46",
   "metadata": {},
   "source": [
    "# GCN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93a504a9-9046-42af-b2b7-6ae01e99381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a17fc-64b5-47ec-b504-12d3623d9830",
   "metadata": {},
   "source": [
    "#### Normalize Adjacency (with self-loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10bce8a-3260-4720-b2b8-f8c38cc7f8af",
   "metadata": {},
   "source": [
    "The equation represents a common operation in **Graph Neural Networks (GNNs)**, specifically for **normalizing the adjacency matrix** with self-loops:\n",
    "\n",
    "$$\n",
    "\\widehat{A} = A + I, \\quad \\widehat{D}^{-1/2} \\widehat{A} \\widehat{D}^{-1/2}\n",
    "$$\n",
    "\n",
    "### Explanation:\n",
    "1. **$\\widehat{A} = A + I$**:  \n",
    "   - Adds self-loops to the adjacency matrix $A$ (where $I$ is the identity matrix).  \n",
    "   - Ensures each node includes its own features during aggregation.  \n",
    "\n",
    "2. **$\\widehat{D}^{-1/2} \\widehat{A} \\widehat{D}^{-1/2}$**:  \n",
    "   - Normalizes $\\widehat{A}$ using the degree matrix $\\widehat{D}$ (diagonal matrix with $\\widehat{D}_{ii} = \\sum_j \\widehat{A}_{ij}$).  \n",
    "   - Symmetric normalization prevents scaling issues in deep GNNs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b048e-bf9c-4b4d-a851-a14115a4b628",
   "metadata": {},
   "source": [
    "## Mathematical Comparison\n",
    "\n",
    "###  GCN (Kipf & Welling)\n",
    "\n",
    "$ H^{(l+1)} = \\sigma \\left( \\hat{D}^{-1/2} \\hat{A} \\hat{D}^{-1/2} H^{(l)} W^{(l)} \\right) $\n",
    "\n",
    "- $\\hat{A} = A + I$: adds self-loops\n",
    "- Normalized symmetric Laplacian: smooths over neighbors\n",
    "- No custom aggregation/message/update — all built into the matrix product\n",
    "\n",
    "---\n",
    "\n",
    "###  General GNN (Message Passing Framework)\n",
    "\n",
    "$ m_v^{(l)} = \\text{AGGREGATE} \\left( \\{ h_u^{(l)} : u \\in N(v) \\} \\right) $\n",
    "\n",
    "$ h_v^{(l+1)} = \\text{UPDATE} \\left( h_v^{(l)}, m_v^{(l)} \\right) $\n",
    "\n",
    "- Modular and expressive\n",
    "- Can support edge features, attention, direction\n",
    "- Used in GAT, GraphSAGE, MPNN, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d697402c-9400-4d2c-aa8f-549d8596ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency(A):\n",
    "    A_hat = A + torch.eye(A.size(0)) # self loop\n",
    "    D_hat = torch.diag(torch.pow(A_hat.sum(1), -0.5))\n",
    "    return D_hat@A_hat@D_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f2ddd-c1a0-4301-a21b-0662b3f7a573",
   "metadata": {},
   "source": [
    "#### Define a Custom GCN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "218bb477-db96-4aee-8ada-5ecaa7ab7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGNNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # Manually define weight matrix\n",
    "        self.W = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        # we use nn.Linear\n",
    "        # self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, X, A_hat):\n",
    "        out = A_hat @ X               # Aggregate neighbor features\n",
    "        out = out @ self.W            # Linear transformation\n",
    "        # out = self.linear(out)      # Linear transformation\n",
    "        return F.relu(out)           # Non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77336d24-fdac-4ffb-83c8-51a769fa2799",
   "metadata": {},
   "source": [
    "#### GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "765d0bd9-229e-46fd-816a-851d75705478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGNN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, out_features):\n",
    "        super(CustomGNN, self).__init__()\n",
    "        self.layer1 = CustomGNNLayer(in_features, hidden_dim)\n",
    "        self.layer2 = CustomGNNLayer(hidden_dim, out_features)\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        A_hat = normalize_adjacency(A)\n",
    "        x = self.layer1(X, A_hat)\n",
    "        x = self.layer2(x, A_hat)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f92bb-deca-4737-b408-392b9bd28b47",
   "metadata": {},
   "source": [
    "#### Dummy Data\n",
    "- Four Nodes\n",
    "- Each node has two attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11f36a94-1d26-497b-86f1-44a28e84a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([\n",
    "    [1.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 1.0],\n",
    "    [0.0, 0.0]\n",
    "])\n",
    "\n",
    "A = torch.tensor([\n",
    "    [0, 1, 0, 0],\n",
    "    [1, 0, 1, 0],\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 0, 1, 0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "labels = torch.tensor([0, 1, 0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fb50fd-4976-4ca2-8b0b-c005786baebc",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82ee5ce9-21a1-4d7a-a8fb-9e46d45b65b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape is: torch.Size([4, 2]) \n",
      " output values are \n",
      " tensor([[0.5163, 0.9618],\n",
      "        [0.6368, 1.1569],\n",
      "        [0.6303, 1.1506],\n",
      "        [0.4792, 0.8618]], grad_fn=<ReluBackward0>)\n",
      "Predictions: tensor([1, 1, 1, 1])\n",
      "Epoch 0 | Loss: 0.7285 | Accuracy: 0.5000\n",
      "Output Shape is: torch.Size([4, 2]) \n",
      " output values are \n",
      " tensor([[0.6887, 0.5920],\n",
      "        [0.8574, 0.7606],\n",
      "        [0.8507, 0.7609],\n",
      "        [0.6507, 0.5935]], grad_fn=<ReluBackward0>)\n",
      "Predictions: tensor([0, 0, 0, 0])\n",
      "Epoch 25 | Loss: 0.6900 | Accuracy: 0.5000\n",
      "Output Shape is: torch.Size([4, 2]) \n",
      " output values are \n",
      " tensor([[0.6651, 0.6086],\n",
      "        [0.8028, 0.7836],\n",
      "        [0.7899, 0.7843],\n",
      "        [0.5918, 0.6125]], grad_fn=<ReluBackward0>)\n",
      "Predictions: tensor([0, 0, 0, 1])\n",
      "Epoch 50 | Loss: 0.6853 | Accuracy: 0.7500\n",
      "Output Shape is: torch.Size([4, 2]) \n",
      " output values are \n",
      " tensor([[0.7297, 0.5841],\n",
      "        [0.8397, 0.7725],\n",
      "        [0.8150, 0.7784],\n",
      "        [0.5898, 0.6175]], grad_fn=<ReluBackward0>)\n",
      "Predictions: tensor([0, 0, 0, 1])\n",
      "Epoch 75 | Loss: 0.6762 | Accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "model = CustomGNN(in_features=2, hidden_dim=4, out_features=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(X, A)\n",
    "    loss = loss_fun(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 25 == 0:\n",
    "        print(f\"Output Shape is: {out.shape} \\n output values are \\n {out}\")\n",
    "        pred = out.argmax(dim=1)\n",
    "        print(f\"Predictions: {pred}\")\n",
    "        acc = (pred == labels).float().mean().item()\n",
    "        print(f\"Epoch {epoch} | Loss: {loss.item():.4f} | Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff085c3-0a3d-4405-94fd-b987e49a69ba",
   "metadata": {},
   "source": [
    "#### **GCN is a special case of a more general GNN framework.**\n",
    "\n",
    "\n",
    "It's efficient and works well for many tasks, but has limitations:\n",
    "- **Can’t handle edge features**\n",
    "- **Assumes all neighbors are equally important**\n",
    "- **Can over-smooth if too deep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af84986d-ef7d-424d-9752-0a75685365af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b33c048-2b9c-4aa1-adb3-c851e8e07457",
   "metadata": {},
   "source": [
    "### GNN vs. GCN — Key Differences\n",
    "\n",
    "| Feature            | General GNN                          | GCN (Kipf & Welling)                       |\n",
    "|---------------------|--------------------------------------|--------------------------------------------|\n",
    "| Aggregation         | User-defined (sum, mean, max, attention, etc.) | Fixed: normalized adjacency $\\hat{A} = D^{-1/2} (A + I) D^{-1/2}$ |\n",
    "| Message Function    | Customizable per edge or node        | Simplified: linear propagation via $\\hat{A} \\cdot X \\cdot W$ |\n",
    "| Weight Sharing      | May differ per edge or node type     | Single global weight matrix per layer      |\n",
    "| Edge Features       | Often supported                      | Not in vanilla GCN                         |\n",
    "| Edge Direction      | Can be directional (e.g., via attention) | Treats edges as undirected                 |\n",
    "| Learned Adjacency   | Optional (e.g., in attention-based GNNs) | Fixed — no learnable adjacency             |\n",
    "| Expressive Power    | Flexible, modular                    | Simpler but efficient and robust           |\n",
    "| Computational Cost  | Higher for complex GNNs              | Low — sparse matrix ops                    |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Model       | Best For                | Key Idea                       |\n",
    "|-------------|-------------------------|--------------------------------|\n",
    "| GCN         | Node/graph classification | Convolution with normalized adjacency |\n",
    "| GraphSAGE   | Large, inductive graphs  | Neighborhood sampling          |\n",
    "| GAT         | Attention on neighbors   | Learnable neighbor importance  |\n",
    "| GIN         | Graph classification    | Strong expressive power        |\n",
    "| MPNN        | Custom models           | Message-passing framework      |\n",
    "| ST-GCN      | Action recognition      | Spatio-temporal structure      |\n",
    "| R-GCN/HAN   | Heterogeneous graphs    | Node/edge types                |\n",
    "\n",
    "\n",
    "### Concept Table\n",
    "\n",
    "| Concept                   | Meaning                          | Why It Matters                  |\n",
    "|---------------------------|----------------------------------|---------------------------------|\n",
    "| $ A @ X $          | Aggregate neighbor features      | Enables message passing         |\n",
    "| $ A + I $               | Add self-loop                    | Include node's own features     |\n",
    "| $ D^{-1/2} A D^{-1/2} $ | Normalize                        | Balance influence of neighbors  |\n",
    "| GCN layer                 | Convolution on graphs            | Simple, effective baseline      |\n",
    "| Custom aggregation (sum/mean/max) | General GNN                | Adds flexibility (like GraphSAGE) |\n",
    "| Stacking layers           | Increases receptive field        | Enables multi-hop propagation   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59535f8e-f52d-4fe5-be50-bf03dcda06a8",
   "metadata": {},
   "source": [
    "# GAT (Graph Attention Network)\n",
    "Introduced by Veličković et al., 2018, GAT uses attention mechanisms to weigh neighbor contributions dynamically.\n",
    "GAT enhances message passing by **assigning different weights** to different neighbors, using **learned attention scores**.\n",
    "\n",
    "\n",
    "\n",
    "### Core Equation:\n",
    "\n",
    "$h_i = \\sigma \\left( \\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij} W h_j \\right)$\n",
    "\n",
    "#### Where:\n",
    "\n",
    "- $W$: learnable weight matrix\n",
    "- $\\alpha_{ij}$: attention coefficient from node $i$ to node $j$\n",
    "- $\\sigma$: non-linearity (e.g., ReLU)\n",
    "\n",
    "### GAT (Node-level attention)\n",
    "\n",
    "| Aspect              | Description                                      |\n",
    "|---------------------|--------------------------------------------------|\n",
    "| Introduced by       | \"Who among my neighbors should I pay more attention to?\" |\n",
    "| Formula             | For a node $v$, GAT computes: <br> $h'_v = \\sigma \\left( \\sum_{u \\in N(v)} \\alpha_{vu} W h_u \\right)$ |\n",
    "| Details             | - $\\alpha_{vu}$: attention weight between node $v$ and neighbor $u$ <br> - Attention is learned per edge using the node features. |\n",
    "\n",
    "---\n",
    "\n",
    "| Aspect              | GAT (Graph Attention Network)               |\n",
    "|---------------------|---------------------------------------------|\n",
    "| Introduced by       | Veličković et al., 2018                     |\n",
    "| Goal                | Learn which neighbors are most important (attend over neighbors) |\n",
    "| Input Graph         | Homogeneous (same node & edge types)        |\n",
    "| Attention           | Over neighbor nodes (per node)              |\n",
    "| Aggregation         | Weighted sum of neighbor features using learned attention |\n",
    "| Adjacency Matrix    | Static (edges fixed, weights learned)       |\n",
    "| Use Case            | Social networks, citation networks, molecular graphs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ef6af6a-0414-4a88-a415-890c00d744a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6714a0d-6c73-4acd-8037-04889c6dd28d",
   "metadata": {},
   "source": [
    "## GAT Layer (Single-head, Single-layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0bcbb13-6577-45a6-b9cc-dc80a8905a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.a = nn.Parameter(torch.randn(2 * out_features, 1))\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        N = X.size(0)\n",
    "\n",
    "        # Linear transform of input features\n",
    "        H = X @ self.W  # [N, out_features]\n",
    "\n",
    "        # Prepare attention inputs (concat h_i || h_j for each edge)\n",
    "        H_repeat_i = H.repeat(1, N).view(N * N, -1)\n",
    "        H_repeat_j = H.repeat(N, 1)\n",
    "        H_cat = torch.cat([H_repeat_i, H_repeat_j], dim=1)  # [N*N, 2*out_features]\n",
    "\n",
    "        # Compute attention scores\n",
    "        e = self.leakyrelu(H_cat @ self.a).view(N, N)  # [N, N]\n",
    "\n",
    "        # Mask non-existent edges\n",
    "        e = e.masked_fill(A == 0, float(\"-inf\"))\n",
    "\n",
    "        # Normalize with softmax\n",
    "        alpha = F.softmax(e, dim=1)  # [N, N]\n",
    "\n",
    "        # Compute new node representations\n",
    "        H_prime = alpha @ H  # [N, out_features]\n",
    "        return F.elu(H_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff780ffd-c610-450e-8801-cd4bd12f8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, out_features):\n",
    "        super(GAT, self).__init__()\n",
    "        self.gat1 = GATLayer(in_features, hidden_dim)\n",
    "        self.gat2 = GATLayer(hidden_dim, out_features)\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        x = self.gat1(X, A)\n",
    "        x = self.gat2(x, A)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fd835c0-af79-4b17-a75b-bedbc4ae42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([\n",
    "    [1.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 1.0],\n",
    "    [0.0, 0.0]\n",
    "])\n",
    "\n",
    "A = torch.tensor([\n",
    "    [1, 1, 0, 0],  # self-loop included\n",
    "    [1, 1, 1, 0],\n",
    "    [0, 1, 1, 1],\n",
    "    [0, 0, 1, 1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "labels = torch.tensor([0, 1, 0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17e3cf13-3bd1-4d7b-90eb-c0086b76ee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 1.3686 | Accuracy: 0.5000\n",
      "Epoch 10 | Loss: 1.1047 | Accuracy: 0.5000\n",
      "Epoch 20 | Loss: 0.9026 | Accuracy: 0.5000\n",
      "Epoch 30 | Loss: 0.7715 | Accuracy: 0.5000\n",
      "Epoch 40 | Loss: 0.7103 | Accuracy: 0.5000\n",
      "Epoch 50 | Loss: 0.6909 | Accuracy: 0.5000\n",
      "Epoch 60 | Loss: 0.6906 | Accuracy: 0.5000\n",
      "Epoch 70 | Loss: 0.6892 | Accuracy: 0.5000\n",
      "Epoch 80 | Loss: 0.6881 | Accuracy: 0.5000\n",
      "Epoch 90 | Loss: 0.6874 | Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = GAT(in_features=2, hidden_dim=4, out_features=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(X, A)\n",
    "    loss = loss_fn(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        pred = out.argmax(dim=1)\n",
    "        acc = (pred == labels).float().mean().item()\n",
    "        print(f\"Epoch {epoch} | Loss: {loss.item():.4f} | Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8099c8fd-7580-4f54-89b4-2d055052b8c0",
   "metadata": {},
   "source": [
    "### Transformer Attention: Quick Recap\n",
    "\n",
    "In the Transformer architecture, attention is computed as:\n",
    "\n",
    "$Attention(Q, K, V) = softmax \\left( \\frac{QK^T}{\\sqrt{d}} \\right) V$\n",
    "\n",
    "### Where:\n",
    "\n",
    "- $Q$ = query vector\n",
    "- $K$ = key vector\n",
    "- $V$ = value vector\n",
    "\n",
    "Each token generates its own $Q$, $K$, and $V$ using learnable linear projections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3467258-5bfb-4ffa-87d5-fd9cadf5ceec",
   "metadata": {},
   "source": [
    "### Why GAT Doesn't Explicitly Use Q/K/V\n",
    "\n",
    "In GAT (Graph Attention Network), the attention is simplified for computational and architectural efficiency:\n",
    "\n",
    "- Each node uses the same transformed feature vector $h_i = W x_i$ as the basis for both query and key.\n",
    "- The attention score is computed using concatenated pairs of node features:\n",
    "\n",
    "  $e_{ij} = LeakyReLU \\left( a^\\top [W h_i || W h_j] \\right)$\n",
    "\n",
    "Instead of:\n",
    "\n",
    "  $e_{ij} = Q_i \\cdot K_j^\\top$\n",
    "\n",
    "So:\n",
    "\n",
    "- GAT collapses $Q$, $K$, $V$ into one space: $h_i = W x_i$\n",
    "- The attention weights $\\alpha_{ij}$ are learned per edge through this concatenation mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc832a-2f1e-4291-9878-c0dc9972cf19",
   "metadata": {},
   "source": [
    "### GAT vs Transformer Attention\n",
    "\n",
    "| Component      | Transformer Attention         | GAT Attention                |\n",
    "|----------------|--------------------------------|------------------------------|\n",
    "| Query          | $Q = W_Q x$                   | Implicit via $W x_i$         |\n",
    "| Key            | $K = W_K x$                   | Implicit via $W x_j$         |\n",
    "| Value          | $V = W_V x$                   | Also $W x_j$ (same as key)   |\n",
    "| Scoring        | Dot product: $Q K^T$          | Concatenation: $a^\\top [W h_i || W h_j]$ |\n",
    "| Weighting      | Softmax of dot product        | Softmax of $e_{ij}$          |\n",
    "| Aggregation    | Weighted sum of $V$           | Weighted sum of $W x_j$      |\n",
    "\n",
    "So even though GAT doesn't explicitly call them $Q$/ $K$/ $V$, conceptually:\n",
    "- Query = node $i$'s own transformed features\n",
    "- Key = neighbors' transformed features\n",
    "- Value = same as Key (neighbors' features)\n",
    "- The attention weights are computed by concatenating the two and passing through a learned vector $a$, instead of dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb82e32b-9316-4e45-ac6c-a65aa9f45f51",
   "metadata": {},
   "source": [
    "# Multi-Head Attention in GAT\n",
    "\n",
    "**Just like in Transformers:**\n",
    "- Multiple attention heads capture different patterns or relationships.\n",
    "- It improves stability and representation power.\n",
    "\n",
    "**Two common ways to combine multi-head outputs:**\n",
    "- Concatenation (used in hidden layers)\n",
    "- Averaging (used in the output layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41931b63-da95-431e-b69f-665540700f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae0b96f9-0432-4938-b55a-5e23e1d45ee7",
   "metadata": {},
   "source": [
    "### GAT Layer (reuse from before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a46dccb7-57b7-4f2c-b3cd-d8338173d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.a = nn.Parameter(torch.randn(2 * out_features, 1))\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        N = X.size(0)\n",
    "        H = X @ self.W  # Linear transformation\n",
    "\n",
    "        H_repeat_i = H.repeat(1, N).view(N * N, -1)\n",
    "        H_repeat_j = H.repeat(N, 1)\n",
    "        H_cat = torch.cat([H_repeat_i, H_repeat_j], dim=1)\n",
    "\n",
    "        e = self.leakyrelu(H_cat @ self.a).view(N, N)\n",
    "        e = e.masked_fill(A == 0, float(\"-inf\"))\n",
    "        alpha = F.softmax(e, dim=1)\n",
    "        H_prime = alpha @ H\n",
    "        return F.elu(H_prime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8c590-e540-405f-88b7-6170affccf15",
   "metadata": {},
   "source": [
    "### Multi-head GAT layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2692f969-2daf-4eaf-9bec-a4746d8b7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, merge='concat'):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.merge = merge\n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            GATLayer(in_features, out_features) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        head_outputs = [attn(X, A) for attn in self.attn_heads]  # [head1, head2, ...]\n",
    "        \n",
    "        if self.merge == 'concat':\n",
    "            return torch.cat(head_outputs, dim=1)  # [N, out_features * num_heads]\n",
    "        elif self.merge == 'mean':\n",
    "            return torch.mean(torch.stack(head_outputs), dim=0)  # [N, out_features]\n",
    "        else:\n",
    "            raise ValueError(\"Merge method must be 'concat' or 'mean'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b4872b-c3a4-44cb-941c-a0e38c02a303",
   "metadata": {},
   "source": [
    "### Full Multi-Head GAT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a0223ee-4fc3-428a-9dd3-abf1b51dc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, out_features, num_heads=4):\n",
    "        super(GAT, self).__init__()\n",
    "        self.gat1 = MultiHeadGATLayer(in_features, hidden_dim, num_heads=num_heads, merge='concat')\n",
    "        self.gat2 = MultiHeadGATLayer(hidden_dim * num_heads, out_features, num_heads=1, merge='mean')\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        x = self.gat1(X, A)\n",
    "        x = self.gat2(x, A)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3dd2300-1389-4732-b1a2-47f6ccd083a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([\n",
    "    [1.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "    [1.0, 1.0],\n",
    "    [0.0, 0.0]\n",
    "])\n",
    "\n",
    "A = torch.tensor([\n",
    "    [1, 1, 0, 0],\n",
    "    [1, 1, 1, 0],\n",
    "    [0, 1, 1, 1],\n",
    "    [0, 0, 1, 1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "labels = torch.tensor([0, 1, 0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a233cacf-990c-4c51-ac3f-cc7b1aeebde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.8488 | Accuracy: 0.5000\n",
      "Epoch 10 | Loss: 0.6743 | Accuracy: 0.5000\n",
      "Epoch 20 | Loss: 0.6393 | Accuracy: 0.7500\n",
      "Epoch 30 | Loss: 0.6212 | Accuracy: 0.5000\n",
      "Epoch 40 | Loss: 0.6089 | Accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "model = GAT(in_features=2, hidden_dim=4, out_features=2, num_heads=4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(X, A)\n",
    "    loss = loss_fn(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        pred = out.argmax(dim=1)\n",
    "        acc = (pred == labels).float().mean().item()\n",
    "        print(f\"Epoch {epoch} | Loss: {loss.item():.4f} | Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873143e6-6566-4b86-a95d-8d83b0b232f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b581e5a-bb9a-480c-82b5-049639bf49d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245223e-a1f6-4672-8875-f03ee6c0a4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc202ee-ada1-49c7-8495-05563fe002ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e854ce9c-4be3-48f8-969d-909d7e8881f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41de419-cea4-4b4e-a9d3-edc483c41c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c43aefc-5a61-477c-9431-d98f2fd93255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13826ffa-c502-4e3c-a3d6-514fed9e1157",
   "metadata": {},
   "source": [
    "# GTN\n",
    "\n",
    "GTN (Graph Transformer Network) — a model for **heterogeneous graphs**, introduced by Yao et al. (2020). It learns **meta-paths** (combinations of relations) automatically using soft selection of edge types.\n",
    "\n",
    "**Learns meaningful meta-paths** between nodes by **softly selecting and composing multiple edge types.**\n",
    "\n",
    "This makes it especially powerful for **heterogeneous node classification**, where the importance of edge types (relations) is **not fixed** and must be learned.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "177017ec-3c72-43a6-86be-e87e15e14b43",
   "metadata": {},
   "source": [
    "## How GTN works\n",
    "\n",
    "### Step 1: Soft Edge-Type Selection\n",
    "\n",
    "Given multiple adjacency matrices $ A_r \\in \\mathbb{R}^{N \\times N} $, GTN learns to softly combine them into a new adjacency matrix $ A^{(l)} $ by:\n",
    "\n",
    "$ A^{(l)} = \\sum_{r=1}^R \\alpha_r^{(l)} A_r $\n",
    "\n",
    "- $ \\alpha_r^{(l)} $: learnable weights (normalized using softmax)\n",
    "- $ A_r $: processed via 1x1 convolution over edge types\n",
    "\n",
    "This selects the first edge in the meta-path.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Learn Longer Meta-Paths\n",
    "\n",
    "To build longer paths like $ A \\rightarrow B \\rightarrow C $, multiply adjacency matrices:\n",
    "\n",
    "$ A^{(l)} = \\alpha^{(l-1)}_1 A^{(l-1)} $\n",
    "\n",
    "This constructs paths of length $ l $. GTN stacks $ l $ such layers to build meta-paths like:\n",
    "\n",
    "$ A^{(l)} = A^{(1)} \\cdot A^{(2)} \\cdot \\ldots \\cdot A^{(l)} $\n",
    "\n",
    "Each $ ( A^{(l)} ) $ is a soft combination of input edge types, so the full path is learned compositionally.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Graph Convolution\n",
    "\n",
    "After constructing the learned adjacency matrix $ A^{(l)} $, perform a standard GCN layer:\n",
    "\n",
    "$ Z = \\sigma(A^{(l)} X W) $\n",
    "\n",
    "- \\( X \\): input node features\n",
    "- \\( W \\): learnable weight matrix\n",
    "- $ A^{(l)} $: normalized learned adjacency matrix\n",
    "- $ \\sigma $: activation function (e.g., ReLU)\n",
    "\n",
    "This gives the final node embeddings, used for tasks like classification.\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Equation\n",
    "\n",
    "Let $A_r \\in \\mathbb{R}^{N \\times N}$ for $r = 1, \\ldots, R$\n",
    "\n",
    "1. **Learned soft edge matrix:**\n",
    "\n",
    "$ A^{(l)} = \\sum_{r=1}^R \\text{softmax}(w_r^{(l)}) \\cdot A_r $\n",
    "\n",
    "2. **Meta-path composition (length-2 simplicity):**\n",
    "\n",
    "$ A^{(2)} = A^{(1)} \\cdot A^{(1)} $\n",
    "\n",
    "3. **Feature propagation:**\n",
    "\n",
    "$ Z = \\text{ReLU}(A^{(l)} X W) $\n",
    "\n",
    "You can extend this to multiple channels (paths) and multiple layers for deeper compositions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b11d4-d511-4084-b602-22bfcbb111d8",
   "metadata": {},
   "source": [
    "## Visualization of GTN Architecture\n",
    "\n",
    "```sql\n",
    "Input:\n",
    "   X  ---> Feature matrix\n",
    "   A1, A2, ..., AR  ---> Multiple adjacency matrices (edge types)\n",
    "\n",
    "1×1 Conv:\n",
    "   Softly selects edge types (learns α1, α2, ..., αR)\n",
    "\n",
    "Adj Multiplication:\n",
    "   Builds meta-paths via matrix multiplication\n",
    "\n",
    "GCN Layer:\n",
    "   Learns node representations from meta-path graph\n",
    "\n",
    "Classifier:\n",
    "   Predicts node classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233dc686-7bde-454a-bc5d-90d861362649",
   "metadata": {},
   "source": [
    "### Why GTN is Powerful\n",
    "- Learns meta-paths without human intuition\n",
    "- Works on heterogeneous graphs with minimal assumptions\n",
    "- Handles different semantics of edge types using attention-like soft selection\n",
    "- Can model longer dependencies via adjacency matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270adce0-fc87-4d10-8d7e-163fc36828cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
