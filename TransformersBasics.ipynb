{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b12df61",
   "metadata": {},
   "source": [
    "#  Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d771b6",
   "metadata": {},
   "source": [
    "### Attention Mechanism Formula\n",
    "\n",
    "The core equation for **scaled dot-product attention** is:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "#### Components:\n",
    "- **\\( Q \\) (Query)**: Represents what the model is \"looking for\" (e.g., a word in a sentence).  \n",
    "- **\\( K \\) (Key)**: Encodes what each element in the input sequence \"offers\" (used to compute relevance).  \n",
    "- **\\( V \\) (Value)**: Contains the actual content to be retrieved if the query matches the key.  \n",
    "- **\\( d_k \\)**: Dimensionality of the key vectors. Scaling by \\( \\sqrt{d_k} \\) prevents gradient vanishing in softmax for large dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition:\n",
    "1. **\\( QK^T \\)**: Computes pairwise similarity scores between queries and keys.  \n",
    "2. **Softmax**: Normalizes scores into probabilities (attention weights).  \n",
    "3. **Weighted Sum (\\( \\cdot V \\))**: Aggregates values based on attention weights.  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Properties:\n",
    "- **Efficiency**: Computes in parallel (unlike RNNs).  \n",
    "- **Interpretability**: Attention weights reveal which inputs the model focuses on.  \n",
    "- **Scalability**: Used in Transformers for tasks like translation (e.g., \"hello\" → \"hola\").  \n",
    "\n",
    "---\n",
    "\n",
    "### Example (PyTorch):\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input tensors (batch_size=1, seq_len=2, d_k=3)\n",
    "Q = torch.tensor([[[1.0, 2.0, 3.0]]])  # Query\n",
    "K = V = torch.tensor([[[4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])  # Key=Value\n",
    "\n",
    "scores = torch.matmul(Q, K.transpose(-1, -2)) / (3 ** 0.5)  # QK^T / sqrt(d_k)\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "output = torch.matmul(weights, V)  # Weighted sum\n",
    "\n",
    "print(\"Attention Output:\", output)\n",
    "```\n",
    "\n",
    "**Output** (weighted combination of values):  \n",
    "```\n",
    "Attention Output: tensor([[[6.9999, 7.9999, 8.9999]]])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Applications:\n",
    "1. **Transformers**: Self-attention in encoder/decoder layers.  \n",
    "2. **Vision**: Image captioning (attending to image regions).  \n",
    "3. **NLP**: BERT, GPT (contextual word representations).  \n",
    "\n",
    "**Reference**: [Vaswani et al. (2017), \"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ac8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Input: 3 words with embedding size 4 (for simplicity)\n",
    "X = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],   # Word 1 (\"I\")\n",
    "    [0.0, 1.0, 1.0, 0.0],    # Word 2 (\"love\")\n",
    "    [1.0, 0.5, 0.0, 1.0]     # Word 3 (\"ice\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64670615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize random weight matrices for Q, K, V\n",
    "d_k = X.shape[1]  # Dimension of embeddings (4)\n",
    "W_Q = torch.randn(d_k, d_k)\n",
    "W_K = torch.randn(d_k, d_k)\n",
    "W_V = torch.randn(d_k, d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ff16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = X @ W_Q.T   # (3, 4)\n",
    "K = X @ W_K.T   # (3, 4)\n",
    "V = X @ W_V.T   # (3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "434cc3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attention\n",
    "scores = Q @ K.T               # (3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d2e760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      " tensor([[ 2.2379,  0.5276, -1.0577,  0.5538],\n",
      "        [ 0.5632, -0.0875, -1.0984, -0.2464],\n",
      "        [ 0.4480, -0.1025, -1.1207, -0.3185]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "scaled_scores = scores / (2 ** 0.5)  # scale by sqrt(d_k)\n",
    "attention_weights = F.softmax(scaled_scores, dim=-1)  # (3, 3)\n",
    "output = attention_weights @ V   # (3, 4)\n",
    "print(\"Attention Output:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ffb04f",
   "metadata": {},
   "source": [
    "## Self-Attention as a PyTorch Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f385fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.W_Q = nn.Linear(embed_size, embed_size)\n",
    "        self.W_K = nn.Linear(embed_size, embed_size)\n",
    "        self.W_V = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Q = self.W_Q(X)\n",
    "        K = self.W_K(X)\n",
    "        V = self.W_V(X)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.embed_size))\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c70b740f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with PyTorch Module:\n",
      " tensor([[-0.8030, -0.0612, -0.5328,  1.0383],\n",
      "        [-0.8797, -0.0825, -0.5166,  1.0086],\n",
      "        [-0.8244, -0.0654, -0.5287,  1.0291]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "attention = SelfAttention(embed_size=4)\n",
    "output = attention(X)\n",
    "print(\"Output with PyTorch Module:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d2a408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc4f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6fd5f43",
   "metadata": {},
   "source": [
    "## Multi Head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b27b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define projection matrices for 2 heads\n",
    "W_q1 = torch.randn(d_k, d_k)\n",
    "W_k1 = torch.randn(d_k, d_k)\n",
    "W_v1 = torch.randn(d_k, d_k)\n",
    "\n",
    "W_q2 = torch.randn(d_k, d_k)\n",
    "W_k2 = torch.randn(d_k, d_k)\n",
    "W_v2 = torch.randn(d_k, d_k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26ca94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head 1\n",
    "Q1 = X @ W_q1.T\n",
    "K1 = X @ W_k1.T\n",
    "V1 = X @ W_v1.T\n",
    "scores1 = Q1 @ K1.T / (2 ** 0.5)\n",
    "attn1 = F.softmax(scores1, dim=-1)\n",
    "out1 = attn1 @ V1\n",
    "\n",
    "# Head 2\n",
    "Q2 = X @ W_q2.T\n",
    "K2 = X @ W_k2.T\n",
    "V2 = X @ W_v2.T\n",
    "scores2 = Q2 @ K2.T / (2 ** 0.5)\n",
    "attn2 = F.softmax(scores2, dim=-1)\n",
    "out2 = attn2 @ V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8c76c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head Attention Output:\n",
      " tensor([[ 0.7570, -0.1414,  1.1894,  0.8090, -0.8680, -0.5871, -1.7324, -1.8429],\n",
      "        [ 0.7806, -0.1700,  1.2400,  0.7191, -0.8476, -0.5985, -1.5904, -1.8747],\n",
      "        [ 0.8430, -0.2384,  1.3093,  0.5228, -0.8673, -0.5944, -1.6964, -1.8339]])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate heads\n",
    "multi_head_output = torch.cat([out1, out2], dim=-1)\n",
    "\n",
    "print(\"Multi-head Attention Output:\\n\", multi_head_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ded46",
   "metadata": {},
   "source": [
    "**Each head sees the data differently — one might focus on similar directions, another on differences, etc.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b866b95d",
   "metadata": {},
   "source": [
    "I have manually set different weight matrices (W_q1, W_q2, etc.) by hand to illustrate how multi-head attention works with different views. But in practice, **we initialize them as learnable parameters** and let the model optimize them during training.\n",
    "\n",
    "When building a real multi-head attention module in PyTorch, we **initialize the projection matrices randomly using nn.Linear** (which includes bias and weight initialization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f98d9",
   "metadata": {},
   "source": [
    "### Multi-Head Attention Mechanism\n",
    "\n",
    "The Multi-Head Attention extends the standard attention mechanism by running multiple attention heads in parallel, allowing the model to jointly attend to information from different representation subspaces. The key equations are:\n",
    "\n",
    "#### **1. Single Attention Head**  \n",
    "For each head \\( i \\):  \n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)  \n",
    "$$\n",
    "\n",
    "- **\\( Q, K, V \\)**: Input Query, Key, and Value matrices.  \n",
    "- **\\( W_i^Q, W_i^K, W_i^V \\)**: Learnable weight matrices for head \\( i \\).  \n",
    "- **Attention**: Scaled dot-product attention:  \n",
    "  $$\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "  $$\n",
    "\n",
    "#### **2. Concatenation and Projection**  \n",
    "All heads are concatenated and linearly projected:  \n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O  \n",
    "$$\n",
    "\n",
    "- **\\( h \\)**: Number of attention heads.  \n",
    "- **\\( W^O \\)**: Learnable output projection matrix.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Intuitions**  \n",
    "1. **Parallel Processing**:  \n",
    "   - Each head learns different attention patterns (e.g., local vs. global dependencies).  \n",
    "   - Example: In translation, one head may focus on syntax, another on semantics.  \n",
    "\n",
    "2. **Dimensionality Management**:  \n",
    "   - Input embeddings are split into \\( h \\) subspaces (typically \\( d_k = d_v = d_{\\text{model}}/h \\)).  \n",
    "\n",
    "3. **Expressiveness**:  \n",
    "   - Multi-head attention captures diverse relationships (e.g., coreference resolution + word order in NLP).  \n",
    "\n",
    "---\n",
    "\n",
    "### **PyTorch Implementation**  \n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=512, h=8):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.d_k = d_model // h\n",
    "        \n",
    "        # Weight matrices\n",
    "        self.W_Q = nn.Linear(d_model, d_model)  # Queries\n",
    "        self.W_K = nn.Linear(d_model, d_model)  # Keys\n",
    "        self.W_V = nn.Linear(d_model, d_model)  # Values\n",
    "        self.W_O = nn.Linear(d_model, d_model)  # Output\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # Linear projections split into h heads\n",
    "        Q = self.W_Q(Q).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        K = self.W_K(K).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        V = self.W_V(V).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention per head\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / (self.d_k ** 0.5)\n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "        head = torch.matmul(weights, V)\n",
    "        \n",
    "        # Concatenate and project\n",
    "        head = head.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.W_O(head)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Multi-Head?**  \n",
    "| **Single-Head**       | **Multi-Head**                     |\n",
    "|------------------------|-----------------------------------|\n",
    "| Single attention focus | Diverse attention patterns        |\n",
    "| Prone to over-smoothing| Robust to noise/ambiguity         |\n",
    "| Limited expressiveness | Captures hierarchical dependencies|\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**  \n",
    "1. **Transformers**:  \n",
    "   - Self-attention in encoder/decoder layers (e.g., BERT, GPT).  \n",
    "2. **Vision**:  \n",
    "   - Vision Transformers (ViT) for image patches.  \n",
    "3. **Speech**:  \n",
    "   - Conformer models combine CNNs + multi-head attention.  \n",
    "\n",
    "**Reference**: [Vaswani et al. (2017), \"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de2acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMultiHeadAttention(nn.modules):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim%num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Output projection after concatenation of all heads\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
