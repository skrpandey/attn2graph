{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-14T20:17:49.740815Z",
     "start_time": "2025-08-14T20:17:49.733345Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T22:08:19.127395Z",
     "start_time": "2025-08-14T22:08:19.116429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def generate_synthetic_data(n_samples=1000, features=5):\n",
    "#     # generate features\n",
    "#     X = torch.randn(n_samples, features)\n",
    "#     # define an equation with variable level of importance\n",
    "#     w0= 3\n",
    "#     w1 = 2\n",
    "#     w2 = 0.5\n",
    "#     # Target: y = 3*x_0 + 2*x_1 + 0.5*x_2 + noise (features 3 and 4 are less important)\n",
    "#     y = w0 * X[:, 0] + w1 * X[:, 1] + w2 * X[:, 2] + 0.1 * np.random.randn(n_samples)\n",
    "#     X = X.to(torch.float32)\n",
    "#     y = y.to(torch.float32)\n",
    "#     y = y.reshape(-1, 1)\n",
    "#     return X,y\n",
    "\n",
    "def generate_synthetic_data(n_samples=1000, n_features=5):\n",
    "    # Generate random features\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    # Target: y = 3*x_0 + 2*x_1 + 0.5*x_2 + noise (features 3 and 4 are less important)\n",
    "    y = 3 * X[:, 0] + 2 * X[:, 1] + 0.5 * X[:, 2] + 0.1 * np.random.randn(n_samples)\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "X, y= generate_synthetic_data(n_samples=1000, n_features=5)"
   ],
   "id": "2947082880564a7f",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# define a self attention layer",
   "id": "685bc8a01599d4b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T22:07:03.571031Z",
     "start_time": "2025-08-14T22:07:03.561037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define a self attention layer\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, attention_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(in_features=input_dim, out_features=attention_dim)\n",
    "        self.key = nn.Linear(in_features=input_dim, out_features=attention_dim)\n",
    "        self.value = nn.Linear(in_features=input_dim, out_features=input_dim)\n",
    "        self.scale = torch.sqrt(torch.tensor(attention_dim, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_dim)\n",
    "        queries = self.query(x)  # (batch_size, attention_dim)\n",
    "        keys = self.key(x)       # (batch_size, attention_dim)\n",
    "        values = self.value(x)   # (batch_size, input_dim)\n",
    "\n",
    "        # compute attention score\n",
    "        scores = torch.matmul(queries, keys.T)/self.scale # batch_size, batche_size\n",
    "        attention_weights = torch.softmax(scores, dim=1) # batch_size, batche_size\n",
    "\n",
    "        # Apply attention to values\n",
    "        attended = torch.matmul(attention_weights, values)    # (batch_size, input_dim)\n",
    "        return attended, attention_weights"
   ],
   "id": "8bf0da31c46058f0",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Define Neural Network with Attention",
   "id": "d1fef0e2bc6b6ba7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T22:07:03.977303Z",
     "start_time": "2025-08-14T22:07:03.965468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AttentionNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=16):\n",
    "        super(AttentionNet, self).__init__()\n",
    "        self.attention = SelfAttention(input_dim, attention_dim=8)\n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply attention\n",
    "        attended, attention_weights = self.attention(x)\n",
    "        # Combine attended features with original input (residual connection)\n",
    "        x = x + attended\n",
    "        # Feedforward layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, attention_weights"
   ],
   "id": "e0a8489069d90886",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Function",
   "id": "d9c03701c8b610ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T22:07:04.845919Z",
     "start_time": "2025-08-14T22:07:04.841261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model,X, y, epochs=1000, lr = 0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred, _attention_weights = model(X)\n",
    "        loss = criterion(Y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ],
   "id": "6b3d8367a70abbe4",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# main execution",
   "id": "2e821999d860b03b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T22:11:00.910413Z",
     "start_time": "2025-08-14T22:11:00.491824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    X, y= generate_synthetic_data(n_samples=1000, n_features=5)\n",
    "\n",
    "    # Initialize model\n",
    "    model = AttentionNet(input_dim=5)\n",
    "    # Train model\n",
    "    train_model(model=model, X=X, y=y, epochs=100)\n",
    "\n",
    "    # evaluate and extract attention weight\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _Y_pred, attention_weights = model(X)\n",
    "    print(f\"Shape of attention weights: {attention_weights.shape}\")\n",
    "    # Compute average attention weights for each feature\n",
    "    avg_attention = attention_weights.mean(dim=0).numpy()\n",
    "    print(f\"Shape of average attention weights: {avg_attention.shape}\")\n",
    "    print(\"\\nAverage Attention Weights for Each Feature:\")\n",
    "    for i, weight in enumerate(avg_attention[:5]):  # Limit to first 5 for clarity\n",
    "        print(f\"Feature {i}: {weight:.4f}\")\n",
    "\n",
    "main()\n",
    "\n",
    "\n"
   ],
   "id": "92d53e8b97240db8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 14.6100\n",
      "Epoch [51/100], Loss: 12.8845\n",
      "Shape of attention weights: torch.Size([1000, 1000])\n",
      "Shape of average attention weights: (1000,)\n",
      "\n",
      "Average Attention Weights for Each Feature:\n",
      "Feature 0: 0.0008\n",
      "Feature 1: 0.0008\n",
      "Feature 2: 0.0009\n",
      "Feature 3: 0.0009\n",
      "Feature 4: 0.0009\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "18806978a605eddd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
